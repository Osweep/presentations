{"cells":[{"cell_type":"code","source":["from delta.tables import DeltaTable\n","# Welcome to your new notebook\n","# Type here in the cell editor to add code!\n","\n","class FabricTable:\n","    def __init__(self, name, workspace_id, lakehouse_id):\n","        self.name = name\n","        self.workspace_id = workspace_id\n","        self.lakehouse_id = lakehouse_id\n","\n","    def get_path(self):\n","        \"\"\"\n","        Get the path to the Delta table.\n","\n","        :return: The path to the Delta table.\n","        \"\"\"\n","        return f\"abfss://{self.workspace_id}@onelake.dfs.fabric.microsoft.com/{self.lakehouse_id}/Tables/{self.name}\"\n","    \n","    def vacuum(self)->None:\n","        \"\"\"\n","        Run VACUUM on the Delta table with a retention period of 7 days.\n","        \n","        :return: None\n","        \"\"\"\n","        try:\n","            # Create DeltaTable object\n","            delta_table = DeltaTable.forPath(spark, self.get_path())\n","\n","            # Run VACUUM with a retention period of 7 days\n","            delta_table.vacuum(retentionHours=168)\n","\n","            print(\"VACUUM completed successfully.\")\n","        except Exception as e:\n","            print(f\"Error running VACUUM: {str(e)}\")\n","\n","    def describe(self)->None:\n","        \"\"\"\n","        Get the schema and other metadata of the Delta table.\n","        \n","        :return: None\n","        \"\"\"\n","        try:\n","            delta_table = DeltaTable.forPath(spark, self.get_path())\n","            schema = delta_table.toDF().schema\n","            print(\"Table schema:\")\n","            print(schema)\n","        except Exception as e:\n","            print(f\"Error describing table: {str(e)}\")\n","\n","    def delete(self, condition):\n","        \"\"\"\n","        Delete data from the Delta table based on a condition.\n","        \n","        :param condition: The condition to delete data.\n","        :return: None\n","        \"\"\"\n","        try:\n","            delta_table = DeltaTable.forPath(spark, self.get_path())\n","            delta_table.delete(condition)\n","            print(\"DELETE completed successfully.\")\n","        except Exception as e:\n","            print(f\"Error running DELETE: {str(e)}\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":-1,"statement_ids":[],"state":"session_starting","livy_statement_state":null,"session_id":null,"normalized_state":"session_starting","queued_time":"2025-03-10T06:43:19.447096Z","session_start_time":"2025-03-10T06:43:19.4485337Z","execution_start_time":null,"execution_finish_time":null,"parent_msg_id":"2c347aec-cf05-4646-887a-ccc81373ef9e"},"text/plain":"StatementMeta(, , -1, SessionStarting, , SessionStarting)"},"metadata":{}}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d5f612f8-453f-4064-b65f-a0fd12d68d18"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{}},"nbformat":4,"nbformat_minor":5}